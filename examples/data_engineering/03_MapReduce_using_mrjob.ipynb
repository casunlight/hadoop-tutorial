{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Using `MRJob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting Dataset\n",
    "\n",
    "The sample dataset we will mainly use (`data/job-data/job-data-2018-09-*.txt`) for this tutorial contains job postings from one of the US job search websites. The data is stored with each row as a JSON document representing a job posting record. \n",
    "\n",
    "The example below shows a sample job postings from the data file. The sample record has been formatted with 4 spaces indentation. In the real file, each record is stored as a JSON document in one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: JSON document of a job posting record*\n",
    "\n",
    "```\n",
    "{\n",
    "    \"industry\": \"Information Technology\", \n",
    "    \"datePosted\": \"2018-09-07\", \n",
    "    \"salaryCurrency\": \"USD\", \n",
    "    \"validThrough\": \"2018-10-07\", \n",
    "    \"empId\": 671932, \n",
    "    \"jobLocation\": {\n",
    "        \"geo\": {\n",
    "            \"latitude\": \"37.7623\", \n",
    "            \"@type\": \"GeoCoordinates\", \n",
    "            \"longitude\": \"-122.4145\"\n",
    "        }, \n",
    "        \"@type\": \"Place\", \n",
    "        \"address\": {\n",
    "            \"postalCode\": \"94110-2042\", \n",
    "            \"addressLocality\": \"San Francisco\", \n",
    "            \"@type\": \"PostalAddress\", \n",
    "            \"addressRegion\": \"CA\", \n",
    "            \"addressCountry\": {\n",
    "                \"@type\": \n",
    "                \"Country\", \n",
    "                \"name\": \"US\"\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    \"estimatedSalary\": {\n",
    "        \"@type\": \"MonetaryAmount\", \n",
    "        \"currency\": \"USD\", \n",
    "        \"value\": {\n",
    "            \"maxValue\": \"202000\", \n",
    "            \"@type\": \"QuantitativeValue\", \n",
    "            \"unitText\": \"YEAR\", \n",
    "            \"minValue\": \"146000\"\n",
    "        }\n",
    "    }, \n",
    "    \"description\": \"<div><em>Generate insights and impact from data</em><em>.</em></div>\\n<br/>\\n<div>\\n<div>We're looking for data scientists to join the Analytics team who are excited about applying their analytical skills to understand our users and influence decision making. If you are naturally data curious, excited about deriving insights from data, and motivated by having impact on the business, we want to hear from you.</div><br/>\\n\\n<div><strong>You will:</strong></div><div>\\n\\n\\n<ul>\\n<li>Work closely with product and business teams to identify important questions and answer them with data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Apply statistical and econometric models on large datasets to: i) measure results and outcomes, ii) identify causal impact and attribution, iii) predict future performance of users or products.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Design, analyze, and interpret the results of experiments.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Drive the collection of new data and the refinement of existing data sources.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Create analyses that tell a \\\"story\\\" focused on insights, not just data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>We're looking for someone with:</strong></div><div>\\n\\n\\n<ul>\\n<li>3+ years experience working with and analyzing large data sets to solve problems.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>A PhD or MS in a quantitative field (e.g., Economics, Statistics, Eng, Natural Sciences, CS).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Expert knowledge of a scientific computing language (such as R or Python) and SQL.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Strong knowledge of statistics and experimental design.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Ability to communicate results clearly and a focus on driving impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>Nice to haves:</strong></div><div>\\n\\n\\n<ul>\\n<li>Prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>You should include these in your application:</strong></div><div>\\n\\n\\n<ul>\\n<li>Resume and LinkedIn profile.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Description of the most interesting data analysis you've done, key findings, and its impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Link to or attachment of code you've written related to data analysis.</li>\\n</ul>\\n\\n</div>\\n</div>\\n<br/>\", \n",
    "    \"hiringOrganization\": {\n",
    "        \"@type\": \"Organization\", \n",
    "        \"sameAs\": \"www.stripe.com\", \n",
    "        \"name\": \"Stripe\"\n",
    "    },\n",
    "    \"@type\": \"JobPosting\", \n",
    "    \"jobId\": 2280174543, \n",
    "    \"@context\": \"http://schema.org\", \n",
    "    \"employmentType\": \"FULL_TIME\", \n",
    "    \"occupationalCategory\": [\n",
    "        \"15-1111.00\", \n",
    "        \"Computer and Information Research Scientists\"\n",
    "    ], \n",
    "    \"title\": \"Data Scientist\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy input data to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir job-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put ../data/job-data/* job-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Protocols For Input & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mrjob` assumes that all data is newline-delimited bytes. Each job has an *input protocol*, an *output protocol*, and an *internal protocol*. These protocols can be changed by overwritting the attributes: `INPUT_PROTOCOL`, `INTERNAL_PROTOCOL`, and `OUTPUT_PROTOCOL`, respectively.\n",
    "\n",
    "The default *input* protocol is `RawValueProtocol`, which just reads in a line as a `str`.\n",
    "The default *output* and *internal* protocols are both `JSONProtocol`, which reads and writes JSON strings separated by a tab character.\n",
    "\n",
    "`JSONValueProtocol` encodes value as a JSON and discard key (key is read in as None). To load the job posting dataset, we can set `INPUT_PROTOCOL = JSONValueProtocol` which automaticall loads input data as Python `dict` objects.\n",
    "\n",
    "For more information, see [Protocols](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#job-protocols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Simple JSON Parser\n",
    "\n",
    "The script below reads the data into `MRTest.mapper` with each record loaded as a Python dict, and generates output of key-value pairs where keys are `jobId` and values are `jobLocation`, which will then be written into output files as JSON documents. Note that no `MRTest.reducer` is provided, this type of jobs are also called *map-only* jobs.\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> <jobId, jobLocation>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`jobId jobLocation`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  `INPUT_PROTOCOL = JSONValueProtocol` allows MRJob to parse JSON documents as python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/1_protocols.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/1_protocols.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "\n",
    "class MRTest(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        yield value.get('jobId', None), value.get('jobLocation', None)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180918.152432.546356\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180918.152432.546356...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs:///user/hadoop/mr-output': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180918.152437.972664\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180918.152437.972664/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8039006073959697496/] [] /tmp/streamjob291647840707618100.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0001\n",
      "  Submitted application application_1537280577176_0001\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0001/\n",
      "  Running job: job_1537280577176_0001\n",
      "  Job job_1537280577176_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0001 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=463148\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323480\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=463148\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12820480\n",
      "\t\tTotal time spent by all map tasks (ms)=12520\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12520\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12520\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1150\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=119\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=1772\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=292872192\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=164626432\n",
      "\t\tVirtual memory (bytes) snapshot=3922911232\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180918.152437.972664...\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180918.152437.972664...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py -r hadoop \\\n",
    "hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering\n",
    "\n",
    "Keys:\n",
    "\n",
    "- Filtering pattern aims to find a subset of data but usually not change the actural records. \n",
    "  - We can set `OUTPUT_PROTOCOL = JSONValueProtocol` to ignore the key field for each record in the output.\n",
    "- Filtering patterns usually don't need a reducer if each record is filtered individually and the evaluation does not depend on other records.\n",
    "- Filtering usually serves as an abstract pattern for some other patterns.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Data cleaning\n",
    "- Events tracking\n",
    "- Records matching\n",
    "- Random sampling\n",
    "- Dataset splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Filtering\n",
    "\n",
    "Simple filtering is often used when data cleaning, events tracking, outliers removing, etc., are needed.\n",
    "\n",
    "**Example**: Find all jobs with titles relavant to *Data Scientist*.\n",
    "\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> [if keyword in title -> <None, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  `OUTPUT_PROTOCOL = JSONValueProtocol` ignores the key field for each record in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.1_simple_filtering.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.1_simple_filtering.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "\n",
    "class MRSimpleFiltering(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def mapper(self, _, value):\n",
    "        title = value.get('title', '').lower()\n",
    "        if title.find('data scientist') > -1:\n",
    "            yield _, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSimpleFiltering.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180918.152527.148705\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/2.hadoop.20180918.152527.148705...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180918.152531.986639\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.152531.986639/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1271355753105562194/] [] /tmp/streamjob663284053151428870.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0002\n",
      "  Submitted application application_1537280577176_0002\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0002/\n",
      "  Running job: job_1537280577176_0002\n",
      "  Job job_1537280577176_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0002 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2928694\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323468\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=2928694\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9750528\n",
      "\t\tTotal time spent by all map tasks (ms)=9522\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9522\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9522\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1150\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=657\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=301789184\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=169869312\n",
      "\t\tVirtual memory (bytes) snapshot=3928018944\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.152531.986639...\n",
      "Removing temp directory /tmp/2.hadoop.20180918.152531.986639...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Sampling\n",
    "\n",
    "Random sampling pattern allows us to create a subset (usually much smaller) of our larger dataset for quick exploration. Thus each record should have an equal probability of being selected. \n",
    "\n",
    "If reproducible is not required, then we can use a random function, e.g.: `random.uniform(a, b)` in python, to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Create a random subset with 10% of the full dataset.\n",
    "\n",
    "We want to pass an argument `fraction` to our `MRJob` script. We can do this by using `MRJob.configure_args()` and `MRJob.add_passthru_arg()` together.\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> [Prob=0.1 -> <None, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - `MRJob.configure_args()` allows user to define arguments for this script \n",
    "  - `MRJob.add_passthru_arg('--fraction', **kwargs)` defines a command-line argument named `fraction`\n",
    "  - To pass a value to `fraction` via command-line arguemnt: `--fraction <value>`\n",
    "  - To use `fraction` in script: `MRJob.options.fraction`\n",
    "  - `MRJob.mapper_init()` validates the value of `fraction` before the `mapper` processes any input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.2_random_sampling.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.2_random_sampling.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class MRRandomSampling(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--fraction', type=float)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.fraction > 1 or self.options.fraction < 0:\n",
    "            raise ValueError('Invalid fraction value')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        key = value.get('jobId', 0)\n",
    "        if random.uniform(0, 1) < self.options.fraction:\n",
    "            yield _, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRRandomSampling.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180918.154235.758049\n",
      "job output is in mr-output/\n",
      "Removing temp directory /tmp/2.hadoop.20180918.154235.758049...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py ../data/job-data/* --output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180918.154240.231092\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.154240.231092/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar840998732318019041/] [] /tmp/streamjob4484908570565239764.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0003\n",
      "  Submitted application application_1537280577176_0003\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0003/\n",
      "  Running job: job_1537280577176_0003\n",
      "  Job job_1537280577176_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0003 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 31\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=936187\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323486\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=936187\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9692160\n",
      "\t\tTotal time spent by all map tasks (ms)=9465\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9465\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9465\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1070\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=176\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=181\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=303669248\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=171442176\n",
      "\t\tVirtual memory (bytes) snapshot=3924807680\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.154240.231092...\n",
      "Removing temp directory /tmp/2.hadoop.20180918.154240.231092...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Splitting\n",
    "\n",
    "For machine learning modeling, we usually divide the data set into two non-overlapping subsets:\n",
    "- training set — a subset to train a model.\n",
    "- test set — a subset to test the trained model.\n",
    "\n",
    "If the goal is to split the dataset into such two subsets, then we need to make sure:\n",
    "- each record can only be selected into one of the two datasets\n",
    "- sampling is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample` function below returns either `True` or `False` based on the hashed value of key and fraction:\n",
    "1. split fraction into *numerator* and *denominator*, e.g.: 0.125 $\\rightarrow$ 125/1000\n",
    "2. calculate the hash value of the key. Here we will use MD5, which is a widely used hash function producing a 128-bit hash value.\n",
    "3. calculate hash value modulo *denominator*, if it's less than *numerator*, return `True`, otherwise return `False`.\n",
    "\n",
    "Note: if you just want to randomly sample the dataset, then a simple random number generator will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "def sample(key, fraction):\n",
    "    if fraction > 1 or fraction < 0:\n",
    "        raise ValueError('Invalid fraction value')\n",
    "    # calculate numerator and denominator\n",
    "    frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "    numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "    denom = 10**(-frac.exponent)\n",
    "    # calculate hash value using md5\n",
    "    hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "    return (int(hash_val, 16) % denom) < numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "# test the function with the code below\n",
    "N = 1000\n",
    "print(sum([sample(i, fraction=0.25) for i in range(N)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Creating a reproducible train/test split.\n",
    " \n",
    "- *Features and highlights*:\n",
    "    \n",
    "  - `MRJob.add_passthru_arg('--split')` defines a command-line argument named `split`, which takes value \"train\" or \"test\". \n",
    "    - If `split=train`, it outputs train subset, otherwise it outputs test subset.\n",
    "  - `MRJob.add_passthru_arg('--test_size')` defines a command-line argument named `test_size`. \n",
    "    - The value should be between 0.0 and 1.0, which represent the proportion of the dataset to include in the test split.\n",
    "  - To create a train/test split, we run the script twice, one with `split=train` and one with `split=test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.3_train_test_splitting.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.3_train_test_splitting.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class MRTrainTestSplit(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--split')\n",
    "        self.add_passthru_arg('--test_size', type=float, default=0.3)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.split not in ('train', 'test'):\n",
    "            raise ValueError('Invalid split value')\n",
    "        if self.options.test_size > 1 or self.options.test_size < 0:\n",
    "            raise ValueError('Invalid test size')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        key = value.get('jobId', 0)\n",
    "        include = self._sample(key=key, fraction=self.options.test_size)\n",
    "        if include ^ (self.options.split=='train'):\n",
    "            yield _, value\n",
    "    \n",
    "    def _sample(self, key, fraction=1):\n",
    "        frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "        numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "        denom = 10**(-frac.exponent)\n",
    "        hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "        return (int(hash_val, 16) % denom) < numer\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTrainTestSplit.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180918.155157.415264\n",
      "job output is in mr-output/train\n",
      "Removing temp directory /tmp/2.hadoop.20180918.155157.415264...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180918.155158.250271\n",
      "job output is in mr-output/test\n",
      "Removing temp directory /tmp/2.hadoop.20180918.155158.250271...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/* \\\n",
    "--output-dir mr-output/train \\\n",
    "--test_size 0.3 \\\n",
    "--split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/* \\\n",
    "--output-dir mr-output/test \\\n",
    "--test_size 0.3 \\\n",
    "--split test \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180918.160512.741754\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.160512.741754/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5739251114290174198/] [] /tmp/streamjob3525669514403577379.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0004\n",
      "  Submitted application application_1537280577176_0004\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0004/\n",
      "  Running job: job_1537280577176_0004\n",
      "  Job job_1537280577176_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0004 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/train\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6282102\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323592\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=6282102\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10555392\n",
      "\t\tTotal time spent by all map tasks (ms)=10308\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10308\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10308\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1380\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=160\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=1270\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=300814336\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=174587904\n",
      "\t\tVirtual memory (bytes) snapshot=3925823488\n",
      "job output is in hdfs:///user/hadoop/mr-output/train\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.160512.741754...\n",
      "Removing temp directory /tmp/2.hadoop.20180918.160512.741754...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180918.160551.530374\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.160551.530374/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7352331347355887718/] [] /tmp/streamjob5649260543556027675.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0005\n",
      "  Submitted application application_1537280577176_0005\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0005/\n",
      "  Running job: job_1537280577176_0005\n",
      "  Job job_1537280577176_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0005 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/test\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2569476\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323588\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=2569476\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10276864\n",
      "\t\tTotal time spent by all map tasks (ms)=10036\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10036\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10036\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1180\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=100\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=502\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=306315264\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=170917888\n",
      "\t\tVirtual memory (bytes) snapshot=3927891968\n",
      "job output is in hdfs:///user/hadoop/mr-output/test\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180918.160551.530374...\n",
      "Removing temp directory /tmp/2.hadoop.20180918.160551.530374...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/train \\\n",
    "    --test_size 0.3 \\\n",
    "    --split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/test \\\n",
    "    --test_size 0.3 \\\n",
    "    --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top N Pattern\n",
    "\n",
    "\n",
    "Keys:\n",
    "\n",
    "- Top n pattern aims to retrieve a relatively small number of records from a large data set according to a ranking scheme specified by user without sorting the entire data set.\n",
    "- The subset needs to be small enough to fit into one single node and thus N should not be too large.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Anomaly detection\n",
    "- Finding the top k records with the lowest/highest values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Top N Values\n",
    "\n",
    "In our data set, each job contains two salaires (or no such fields if it's not available), `minValue` and `maxValue`. We want to find out the top N Salaries from all jobs. \n",
    "\n",
    "- To find the top n list across the entire dataset, we need to compare the values from all records, which means the key field becomes less useful and therefore we can assign `None` as the key, which will end up with one single reducer process the reduce job.\n",
    "\n",
    "- To minimize the burden of the reducer and to maximize the parallelism, we can use a combiner to find a local top n list in each mapper container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Find the top N salaries from all jobs. \n",
    "\n",
    "To receive the top N salary list, we first compair `maxValue`, then `minValue`. \n",
    "\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> <_, (maxValue, minValue)>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - combiner:`<_, local_top_n[(maxValue, minValue)]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - reducer:`<_, global_top_n[(maxValue, minValue)]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`top_n[(maxValue, minValue)]`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - Ignore key field in `mapper` so only one reducer will be involved  \n",
    "  - `MRJob.reducer_init()` initializes top N sorted list\n",
    "  - `MRJob.reducer()` inserts record into top N sorted list, alway truncate list to a length of N\n",
    "  - `MRJob.reducer_final()` emits record from top N sorted list\n",
    "  - Add `combiner_init`/`combiner`/`combiner_final` steps through `MrJob.steps()` to reduce data flow\n",
    "  - To maintain a sorted list of size n, we use Python built-in heap sort algorithm to achieve O(log(n)) for each insertion operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/3.1_top_n_value.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/3.1_top_n_value.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import heapq\n",
    "\n",
    "\n",
    "class MRTopNValue(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--top_n', type=int)\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            yield _, (max_, min_)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        if self.options.top_n < 1:\n",
    "            raise ValueError('Invalid top_n value')\n",
    "        self.top_n = []\n",
    "        \n",
    "    def reducer(self, _, values):\n",
    "        for value in values:\n",
    "            if len(self.top_n) < self.options.top_n:\n",
    "                heapq.heappush(self.top_n, value)\n",
    "            else:\n",
    "                heapq.heappushpop(self.top_n, value)\n",
    "                \n",
    "    def reducer_final(self):\n",
    "        for value in self.top_n:\n",
    "            yield None, value\n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       combiner_init=self.reducer_init,\n",
    "                       combiner=self.reducer,\n",
    "                       combiner_final=self.reducer_final,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.reducer,\n",
    "                       reducer_final=self.reducer_final)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNValue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/3.hadoop.20180918.165434.220892\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/3.hadoop.20180918.165434.220892...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py ../data/job-data/* --output-dir mr-output --top_n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/3.hadoop.20180918.171944.000093\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180918.171944.000093/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar369533625508089501/] [] /tmp/streamjob8345320719200866152.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0007\n",
      "  Submitted application application_1537280577176_0007\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0007/\n",
      "  Running job: job_1537280577176_0007\n",
      "  Job job_1537280577176_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1537280577176_0007 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=220\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=565\n",
      "\t\tFILE: Number of bytes written=488923\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=220\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10021888\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3678208\n",
      "\t\tTotal time spent by all map tasks (ms)=9787\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9787\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3592\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3592\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9787\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3592\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1730\n",
      "\t\tCombine input records=972\n",
      "\t\tCombine output records=20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=146\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output bytes=24362\n",
      "\t\tMap output materialized bytes=571\n",
      "\t\tMap output records=972\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=674902016\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=571\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=467140608\n",
      "\t\tVirtual memory (bytes) snapshot=5883355136\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180918.171944.000093...\n",
      "Removing temp directory /tmp/3.hadoop.20180918.171944.000093...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/ --top_n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Top N Records\n",
    "\n",
    "With the top N Salaries being calculated in file `mr-output/part-00000`, now we want to find out which jobs offer those salaries. \n",
    "\n",
    "Since the top N values are considered small enough to fit into each node, it is preferable to distribute the file into those node where the mapper jobs run, rather than load data in the job configuration. This mechanism is called `Distributed Cache`. We can do it via `MRJob.add_file_arg()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Find all jobs that offer salaries from the top N salaries list. \n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - mapper_init: fetch `top_n_list` to each mapper\n",
    "  - $\\quad\\downarrow$\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record>[if record contains salaries from top_n_list -> <_, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - `MRJob.add_file_arg('--cache')` sends an external file to Hadoop\n",
    "  - To send a file to `cache` via command-line: `--cache <file_path>`\n",
    "  - The cached file can then be accessd in script via: `MRJob.options.cache`\n",
    "  - `MRJob.mapper_init()` fetches the top n list from the cached file to each mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/3.2_top_n_job.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/3.2_top_n_job.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "class MRTopNJob(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_file_arg('--cache')\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.cache = list()\n",
    "        with open(self.options.cache, 'r') as f:\n",
    "            for line in f:\n",
    "                self.cache.append(tuple(json.loads(line)))\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            if (max_, min_) in self.cache:\n",
    "                yield _, value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/3.hadoop.20180918.172132.991163\n",
      "job output is in mr-output-jobs\n",
      "Removing temp directory /tmp/3.hadoop.20180918.172132.991163...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.2_top_n_job.py ../data/job-data/* --output-dir mr-output-jobs --cache mr-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs:///user/hadoop/mr-output-jobs': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/3.hadoop.20180918.172138.854476\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180918.172138.854476/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7958850948417445614/] [] /tmp/streamjob307580746471035229.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0008\n",
      "  Submitted application application_1537280577176_0008\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0008/\n",
      "  Running job: job_1537280577176_0008\n",
      "  Job job_1537280577176_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537280577176_0008 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output-jobs\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49855\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=324020\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=49855\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9997312\n",
      "\t\tTotal time spent by all map tasks (ms)=9763\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9763\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9763\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1000\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=138\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=320520192\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=179306496\n",
      "\t\tVirtual memory (bytes) snapshot=3923214336\n",
      "job output is in hdfs:///user/hadoop/mr-output-jobs\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180918.172138.854476...\n",
      "Removing temp directory /tmp/3.hadoop.20180918.172138.854476...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.2_top_n_job.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output-jobs/ \\\n",
    "    --cache hdfs:///user/hadoop/mr-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inverted Index\n",
    "\n",
    "- Keys:\n",
    "  - Indexing is a technique that is used frequently in almost all seach engine systems. Without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power.\n",
    "  - Inverted index is an index data structure storing a mapping from content, such as keywords, to its locations in a database file, or in a document or a set of documents. \n",
    "  - The purpose of an inverted index is to allow fast full text searches, at a cost of increased processing when a document is added to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Generate an index from the data set to allow for faster searches on technique skills.\n",
    "\n",
    "The skills we want to search for are listed below:\n",
    "\n",
    "```\n",
    "skillset = ['Java', 'JavaScript', 'C', 'C++', 'C#', 'Python', 'R', 'Bash',\n",
    "            'MySQL', 'Postgresql', 'MongoDB', 'Html', 'Ruby', 'PHP', \n",
    "            'Swift', 'CSS', 'Julia', 'Golang', 'Github', 'Redis', 'Hadoop',\n",
    "            'Spark', 'Hive', 'Pig', 'Spark', 'ElasticSearch', 'Kafka', \n",
    "            'Cassandra', 'AWS', 'GCP', 'Azure', 'Docker', 'kubernetes']\n",
    "```\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - mapper_init: define search regex pattern\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> find matching skills using regex -> <skill, jobId>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - reducer:`<skill, [jobId]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`skill, [jobId]`\n",
    "  \n",
    "Note: Since no aggregation is performed throughout the entire MapReduce pipeline, a combiner in this example is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/4_inverted_index.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/4_inverted_index.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "class MRIndexingSkills(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    skillset = ['Java', 'JavaScript', 'C', 'C++', 'C#', 'Python', 'R', 'Bash',\n",
    "                'MySQL', 'Postgresql', 'MongoDB', 'Html', 'Ruby', 'PHP', \n",
    "                'Swift', 'CSS', 'Julia', 'Golang', 'Github', 'Redis', 'Hadoop',\n",
    "                'Spark', 'Hive', 'Pig', 'Spark', 'ElasticSearch', 'Kafka', \n",
    "                'Cassandra', 'AWS', 'GCP', 'Azure', 'Docker', 'kubernetes']\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.pattern = re.compile('|'.join(['(?<=\\W){}(?=\\W)'.format(re.escape(x)) for x in self.skillset]), \n",
    "                                  flags=re.IGNORECASE)\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            jobId, description = value['jobId'], value['description']\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            skills = self.pattern.findall(description)\n",
    "            for skill in skills:\n",
    "                yield skill.capitalize(), jobId\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        yield key, list(values)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRIndexingSkills.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/4_inverted_index.hadoop.20180918.190009.189437\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/4_inverted_index.hadoop.20180918.190009.189437...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/4_inverted_index.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/4_inverted_index.hadoop.20180918.184113.520202\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/4_inverted_index.hadoop.20180918.184113.520202/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7840362551436171095/] [] /tmp/streamjob3798702398271974150.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537280577176_0010\n",
      "  Submitted application application_1537280577176_0010\n",
      "  The url to track the job: http://ebee9e947bed:8088/proxy/application_1537280577176_0010/\n",
      "  Running job: job_1537280577176_0010\n",
      "  Job job_1537280577176_0010 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1537280577176_0010 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=60468\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=60899\n",
      "\t\tFILE: Number of bytes written=609786\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=60468\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17382400\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3879936\n",
      "\t\tTotal time spent by all map tasks (ms)=16975\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16975\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3789\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3789\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16975\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3789\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2090\n",
      "\t\tCombine input records=5009\n",
      "\t\tCombine output records=59\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output bytes=90635\n",
      "\t\tMap output materialized bytes=60905\n",
      "\t\tMap output records=5009\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=669650944\n",
      "\t\tReduce input groups=30\n",
      "\t\tReduce input records=59\n",
      "\t\tReduce output records=30\n",
      "\t\tReduce shuffle bytes=60905\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=118\n",
      "\t\tTotal committed heap usage (bytes)=469237760\n",
      "\t\tVirtual memory (bytes) snapshot=5887938560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/4_inverted_index.hadoop.20180918.184113.520202...\n",
      "Removing temp directory /tmp/4_inverted_index.hadoop.20180918.184113.520202...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/4_inverted_index.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
