{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Using `MRJob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting Dataset\n",
    "\n",
    "The sample dataset we will use (`data/job-data/job-data-2018-09-08-00-00-37.txt`) contains job postings from on one of the US job search websites. The data is stored with each row as a JSON document representing a job posting record. \n",
    "\n",
    "The example below shows a sample job postings from the data file. The sample record has been formatted with 4 spaces indentation. In the real file, each record is stored as a JSON document in one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: JSON document of a job posting record*\n",
    "\n",
    "```\n",
    "{\n",
    "    \"industry\": \"Information Technology\", \n",
    "    \"datePosted\": \"2018-09-07\", \n",
    "    \"salaryCurrency\": \"USD\", \n",
    "    \"validThrough\": \"2018-10-07\", \n",
    "    \"empId\": 671932, \n",
    "    \"jobLocation\": {\n",
    "        \"geo\": {\n",
    "            \"latitude\": \"37.7623\", \n",
    "            \"@type\": \"GeoCoordinates\", \n",
    "            \"longitude\": \"-122.4145\"\n",
    "        }, \n",
    "        \"@type\": \"Place\", \n",
    "        \"address\": {\n",
    "            \"postalCode\": \"94110-2042\", \n",
    "            \"addressLocality\": \"San Francisco\", \n",
    "            \"@type\": \"PostalAddress\", \n",
    "            \"addressRegion\": \"CA\", \n",
    "            \"addressCountry\": {\n",
    "                \"@type\": \n",
    "                \"Country\", \n",
    "                \"name\": \"US\"\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    \"estimatedSalary\": {\n",
    "        \"@type\": \"MonetaryAmount\", \n",
    "        \"currency\": \"USD\", \n",
    "        \"value\": {\n",
    "            \"maxValue\": \"202000\", \n",
    "            \"@type\": \"QuantitativeValue\", \n",
    "            \"unitText\": \"YEAR\", \n",
    "            \"minValue\": \"146000\"\n",
    "        }\n",
    "    }, \n",
    "    \"description\": \"<div><em>Generate insights and impact from data</em><em>.</em></div>\\n<br/>\\n<div>\\n<div>We're looking for data scientists to join the Analytics team who are excited about applying their analytical skills to understand our users and influence decision making. If you are naturally data curious, excited about deriving insights from data, and motivated by having impact on the business, we want to hear from you.</div><br/>\\n\\n<div><strong>You will:</strong></div><div>\\n\\n\\n<ul>\\n<li>Work closely with product and business teams to identify important questions and answer them with data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Apply statistical and econometric models on large datasets to: i) measure results and outcomes, ii) identify causal impact and attribution, iii) predict future performance of users or products.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Design, analyze, and interpret the results of experiments.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Drive the collection of new data and the refinement of existing data sources.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Create analyses that tell a \\\"story\\\" focused on insights, not just data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>We're looking for someone with:</strong></div><div>\\n\\n\\n<ul>\\n<li>3+ years experience working with and analyzing large data sets to solve problems.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>A PhD or MS in a quantitative field (e.g., Economics, Statistics, Eng, Natural Sciences, CS).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Expert knowledge of a scientific computing language (such as R or Python) and SQL.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Strong knowledge of statistics and experimental design.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Ability to communicate results clearly and a focus on driving impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>Nice to haves:</strong></div><div>\\n\\n\\n<ul>\\n<li>Prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>You should include these in your application:</strong></div><div>\\n\\n\\n<ul>\\n<li>Resume and LinkedIn profile.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Description of the most interesting data analysis you've done, key findings, and its impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Link to or attachment of code you've written related to data analysis.</li>\\n</ul>\\n\\n</div>\\n</div>\\n<br/>\", \n",
    "    \"hiringOrganization\": {\n",
    "        \"@type\": \"Organization\", \n",
    "        \"sameAs\": \"www.stripe.com\", \n",
    "        \"name\": \"Stripe\"\n",
    "    },\n",
    "    \"@type\": \"JobPosting\", \n",
    "    \"jobId\": 2280174543, \n",
    "    \"@context\": \"http://schema.org\", \n",
    "    \"employmentType\": \"FULL_TIME\", \n",
    "    \"occupationalCategory\": [\n",
    "        \"15-1111.00\", \n",
    "        \"Computer and Information Research Scientists\"\n",
    "    ], \n",
    "    \"title\": \"Data Scientist\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Protocols For Input & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mrjob` assumes that all data is newline-delimited bytes. Each job has an *input protocol*, an *output protocol*, and an *internal protocol*. These protocols can be changed by overwritting the attributes: `INPUT_PROTOCOL`, `INTERNAL_PROTOCOL`, and `OUTPUT_PROTOCOL`, respectively.\n",
    "\n",
    "The default *input* protocol is `RawValueProtocol`, which just reads in a line as a `str`.\n",
    "The default *output* and *internal* protocols are both `JSONProtocol`, which reads and writes JSON strings separated by a tab character.\n",
    "\n",
    "`JSONValueProtocol` encodes value as a JSON and discard key (key is read in as None). To load the job posting dataset, we can set `INPUT_PROTOCOL = JSONValueProtocol` which automaticall loads input data as Python `dict` objects.\n",
    "\n",
    "For more information, see [Protocols](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#job-protocols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: The script below loads the data into `MRTest.mapper` and generates output of key-value pairs where keys are `jobId:int` and values are `jobLocation:dict`, which will then be written into output files as JSON documents. Note that no `MRTest.reducer` is provided, this type of jobs are sometimes called *map-only* jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/1_protocols.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/1_protocols.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol, TextProtocol\n",
    "\n",
    "class MRTest(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        yield value.get('jobId', None), value.get('jobLocation', None)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180914.183728.393372\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180914.183728.393372...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py ../data/job-data/ --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs:///user/hadoop/mr-output': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/1_protocols.hadoop.20180914.183752.023497\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180914.183752.023497/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4781646709125021108/] [] /tmp/streamjob3101220794703745066.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0010\n",
      "  Submitted application application_1536772908775_0010\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0010/\n",
      "  Running job: job_1536772908775_0010\n",
      "  Job job_1536772908775_0010 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1536772908775_0010 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=232696\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323482\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=232696\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8045568\n",
      "\t\tTotal time spent by all map tasks (ms)=7857\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7857\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7857\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1010\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=107\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output records=887\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=332013568\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=176160768\n",
      "\t\tVirtual memory (bytes) snapshot=3923628032\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/1_protocols.hadoop.20180914.183752.023497...\n",
      "Removing temp directory /tmp/1_protocols.hadoop.20180914.183752.023497...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/1_protocols.py -r hadoop \\\n",
    "hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering\n",
    "\n",
    "Keys:\n",
    "\n",
    "- Filtering pattern aims to find a subset of data but (often) not change the actural records. \n",
    "  - We can set `OUTPUT_PROTOCOL = JSONValueProtocol` to ignore the key field for each record in the output.\n",
    "- Filtering patterns usually don't need a reducer if each record is filtered individually and the evaluation does not depend on other records.\n",
    "- Filtering usually serves as an abstract pattern for some other patterns.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Data cleaning\n",
    "- Events tracking\n",
    "- Records matching\n",
    "- Random sampling\n",
    "- Dataset splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Filtering\n",
    "\n",
    "Simple filtering is often used when data cleaning, events tracking, outliers removing, etc. are needed.\n",
    "\n",
    "**Example**: Find all jobs with titles relavant to *Data Scientist*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.1_simple_filtering.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.1_simple_filtering.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "class MRSimpleFiltering(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def mapper(self, _, value):\n",
    "        title = value.get('title', '').lower()\n",
    "        if title.find('data scientist') > -1:\n",
    "            yield _, value\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRSimpleFiltering.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180914.183939.999036\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/2.hadoop.20180914.183939.999036...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180914.184104.444184\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.184104.444184/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4384047019595041235/] [] /tmp/streamjob7867418860001505140.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0012\n",
      "  Submitted application application_1536772908775_0012\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0012/\n",
      "  Running job: job_1536772908775_0012\n",
      "  Job job_1536772908775_0012 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1536772908775_0012 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1731799\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323470\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=1731799\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10096640\n",
      "\t\tTotal time spent by all map tasks (ms)=9860\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9860\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9860\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1160\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output records=372\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=313237504\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=152567808\n",
      "\t\tVirtual memory (bytes) snapshot=3929440256\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.184104.444184...\n",
      "Removing temp directory /tmp/2.hadoop.20180914.184104.444184...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.1_simple_filtering.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Sampling\n",
    "\n",
    "Random sampling pattern allows us to create a subset (usually much smaller) of our larger dataset for quick exploration. Thus each record should have an equal probability of being selected. \n",
    "\n",
    "If reproducible is not required, then we can use a random function, e.g.: `random.uniform(a, b)` in python, to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to pass an argument `fraction` to our `MRJob` class. We can do this by using `MRJob.configure_args()` and `MRJob.add_passthru_arg` together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Create a random subset with 10% of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.2_random_sampling.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.2_random_sampling.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class MRRandomSampling(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('-f', '--fraction', type=float)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.fraction > 1 or self.options.fraction < 0:\n",
    "            raise ValueError('Invalid fraction value')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        key = value.get('jobId', 0)\n",
    "        if random.uniform(0, 1) < self.options.fraction:\n",
    "            yield _, value\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRRandomSampling.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180914.202619.288833\n",
      "job output is in mr-output/\n",
      "Removing temp directory /tmp/2.hadoop.20180914.202619.288833...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py ../data/job-data/ --output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180914.202623.769842\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.202623.769842/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1457984388549201326/] [] /tmp/streamjob3508481496319194522.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0017\n",
      "  Submitted application application_1536772908775_0017\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0017/\n",
      "  Running job: job_1536772908775_0017\n",
      "  Job job_1536772908775_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1536772908775_0017 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=503158\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323486\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=503158\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8832000\n",
      "\t\tTotal time spent by all map tasks (ms)=8625\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8625\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8625\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1140\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=150\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=313786368\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=172490752\n",
      "\t\tVirtual memory (bytes) snapshot=3925164032\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.202623.769842...\n",
      "Removing temp directory /tmp/2.hadoop.20180914.202623.769842...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.2_random_sampling.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "--output-dir mr-output/ --fraction .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Splitting\n",
    "\n",
    "For machine learning modeling, we usually divide the data set into two non-overlapping subsets:\n",
    "- training set — a subset to train a model.\n",
    "- test set — a subset to test the trained model.\n",
    "\n",
    "If the goal is to split the dataset into such two subsets, then we need to make sure:\n",
    "- each record can only be selected into one of the two datasets\n",
    "- sampling is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample` function below return either `True` or `False` based on the key and fraction:\n",
    "1. split fraction into *numerator* and *denominator*, e.g.: 0.125 -> 125/1000\n",
    "2. calculate the hash value of the key. Here we will use MD5, which is a widely used hash function producing a 128-bit hash value.\n",
    "3. calculate hash value modulo *denominator*, if it's less than *numerator*, return `True`, otherwise return `False`.\n",
    "\n",
    "Note: if you just want to randomly sample the dataset, then a simple random number generator will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "def sample(key, fraction):\n",
    "    if fraction > 1 or fraction < 0:\n",
    "        raise ValueError('Invalid fraction value')\n",
    "    # calculate numerator and denominator\n",
    "    frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "    numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "    denom = 10**(-frac.exponent)\n",
    "    # calculate hash value using md5\n",
    "    hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "    return (int(hash_val, 16) % denom) < numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "# test the function with the code below\n",
    "N = 1000\n",
    "print(sum([sample(i, fraction=0.25) for i in range(N)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Randomly slicing a single data set into a training set (70%) and test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/2.3_train_test_splitting.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/2.3_train_test_splitting.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import decimal\n",
    "import hashlib\n",
    "\n",
    "class MRTrainTestSplit(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('-s', '--split', )\n",
    "        self.add_passthru_arg('-t', '--test_size', type=float)\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        if self.options.split not in ('train', 'test'):\n",
    "            raise ValueError('Invalid split value')\n",
    "        if self.options.test_size > 1 or self.options.test_size < 0:\n",
    "            raise ValueError('Invalid test size')\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        key = value.get('jobId', 0)\n",
    "        include = self._sample(key=key, fraction=self.options.test_size)\n",
    "        if include ^ (self.options.split=='train'):\n",
    "            yield _, value\n",
    "    \n",
    "    def _sample(self, key, fraction=1):\n",
    "        frac = decimal.Decimal(str(fraction)).as_tuple()\n",
    "        numer = sum([v*10**i for i, v in enumerate(frac.digits[::-1])])\n",
    "        denom = 10**(-frac.exponent)\n",
    "        hash_val = hashlib.md5(str(key).encode()).hexdigest()\n",
    "        return (int(hash_val, 16) % denom) < numer\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRTrainTestSplit.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180914.205605.794433\n",
      "job output is in mr-output/train\n",
      "Removing temp directory /tmp/2.hadoop.20180914.205605.794433...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/2.hadoop.20180914.205606.327740\n",
      "job output is in mr-output/test\n",
      "Removing temp directory /tmp/2.hadoop.20180914.205606.327740...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/ \\\n",
    "--output-dir mr-output/train \\\n",
    "--test_size 0.3 \\\n",
    "--split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py ../data/job-data/ \\\n",
    "--output-dir mr-output/test \\\n",
    "--test_size 0.3 \\\n",
    "--split test \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Create a reproducible train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180914.205751.274286\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.205751.274286/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8384516427279074643/] [] /tmp/streamjob1112618257944548065.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0018\n",
      "  Submitted application application_1536772908775_0018\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0018/\n",
      "  Running job: job_1536772908775_0018\n",
      "  Job job_1536772908775_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1536772908775_0018 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/train\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3117905\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323592\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=3117905\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8675328\n",
      "\t\tTotal time spent by all map tasks (ms)=8472\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8472\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8472\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1150\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output records=629\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=304697344\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=169869312\n",
      "\t\tVirtual memory (bytes) snapshot=3930836992\n",
      "job output is in hdfs:///user/hadoop/mr-output/train\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.205751.274286...\n",
      "Removing temp directory /tmp/2.hadoop.20180914.205751.274286...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/2.hadoop.20180914.205825.897269\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.205825.897269/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1719021691194138852/] [] /tmp/streamjob8406968112971128818.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0019\n",
      "  Submitted application application_1536772908775_0019\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0019/\n",
      "  Running job: job_1536772908775_0019\n",
      "  Job job_1536772908775_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1536772908775_0019 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/test\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1389774\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=323588\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=1389774\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10857472\n",
      "\t\tTotal time spent by all map tasks (ms)=10603\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10603\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10603\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1020\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=143\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output records=258\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=323702784\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=170393600\n",
      "\t\tVirtual memory (bytes) snapshot=3927298048\n",
      "job output is in hdfs:///user/hadoop/mr-output/test\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/2.hadoop.20180914.205825.897269...\n",
      "Removing temp directory /tmp/2.hadoop.20180914.205825.897269...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/train \\\n",
    "    --test_size 0.3 \\\n",
    "    --split train \\\n",
    "&& python3 mr-jobs/2.3_train_test_splitting.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/test \\\n",
    "    --test_size 0.3 \\\n",
    "    --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top N Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top N algorithm is the top-n items of a dataset. The top ten pattern is a bit different than previous ones in that you know how many records you want to get in the end, no matter what the input size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Top N Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr-jobs/3.1_top_n_value.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/3.1_top_n_value.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import heapq\n",
    "\n",
    "class MRTopNValue(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('-n', '--top_n', type=int)\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            yield _, (max_, min_)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        if self.options.top_n < 1:\n",
    "            raise ValueError('Invalid top_n value')\n",
    "        self.top_n = []\n",
    "        \n",
    "    def reducer(self, _, values):\n",
    "        for value in values:\n",
    "            if len(self.top_n) < self.options.top_n:\n",
    "                heapq.heappush(self.top_n, value)\n",
    "            else:\n",
    "                heapq.heappushpop(self.top_n, value)\n",
    "                \n",
    "    def reducer_final(self):\n",
    "        for value in self.top_n:\n",
    "            yield None, value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNValue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/3.hadoop.20180914.210728.290722\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/3.hadoop.20180914.210728.290722...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py ../data/job-data/ --output-dir mr-output --top_n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/3.hadoop.20180914.210816.817767\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180914.210816.817767/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar745154766757230548/] [] /tmp/streamjob4231415285271400797.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1536772908775_0021\n",
      "  Submitted application application_1536772908775_0021\n",
      "  The url to track the job: http://653a9ad8c076:8088/proxy/application_1536772908775_0021/\n",
      "  Running job: job_1536772908775_0021\n",
      "  Job job_1536772908775_0021 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1536772908775_0021 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4510888\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2198\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13733\n",
      "\t\tFILE: Number of bytes written=513963\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4511142\n",
      "\t\tHDFS: Number of bytes written=2198\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9181184\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3759104\n",
      "\t\tTotal time spent by all map tasks (ms)=8966\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8966\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3671\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3671\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8966\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3671\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1700\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=193\n",
      "\t\tInput split bytes=254\n",
      "\t\tMap input records=887\n",
      "\t\tMap output bytes=12715\n",
      "\t\tMap output materialized bytes=13739\n",
      "\t\tMap output records=506\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=668123136\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=506\n",
      "\t\tReduce output records=100\n",
      "\t\tReduce shuffle bytes=13739\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1012\n",
      "\t\tTotal committed heap usage (bytes)=463470592\n",
      "\t\tVirtual memory (bytes) snapshot=5890707456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180914.210816.817767...\n",
      "Removing temp directory /tmp/3.hadoop.20180914.210816.817767...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/ --top_n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mr-jobs/3.2_top_n_job.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "class MRTopNJob(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('-n', '--top_n', type=int)\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        for cache_file in os.listdir(self.cache_dir):\n",
    "            if cache_file.find('part-') == 0:\n",
    "                with open(os.path.join(self.cache_dir, cache_file), 'r') as f:\n",
    "                    for line in f:\n",
    "                        self.cache.append(tuple(json.loads(line)))\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            if (max_, min_) in self.cache:\n",
    "                yield _, value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 mr-jobs/3.1_top_n_job.py ../data/job-data/job-data-2018-09-08-00-00-37.txt --output-dir mr-output-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tuple(json.loads('[1,2]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('mr-output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True ^ False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
